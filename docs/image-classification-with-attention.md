# æ³¨æ„åŠ›å›¾åƒåˆ†ç±»

> åŸæ–‡ï¼š<https://blog.paperspace.com/image-classification-with-attention/>

å›¾åƒåˆ†ç±»å¯èƒ½æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€æµè¡Œçš„å­é¢†åŸŸä¹‹ä¸€ã€‚å›¾åƒåˆ†ç±»çš„è¿‡ç¨‹åŒ…æ‹¬ç†è§£å›¾åƒä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä»¥å°†å®ƒä»¬åˆ†ç±»åˆ°ä¸€ç»„é¢„å®šä¹‰çš„æ ‡ç­¾ä¸­ã€‚ä½œä¸ºä¸€ä¸ªé¢†åŸŸï¼Œå›¾åƒåˆ†ç±»åœ¨ç¬¬ä¸€æ¬¡ ImageNet æŒ‘æˆ˜èµ›åå˜å¾—å¾ˆæœ‰åï¼Œå› ä¸ºåºå¤§çš„ ImageNet æ•°æ®é›†çš„æ–°é¢–å¯ç”¨æ€§ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„æˆåŠŸä»¥åŠå®ƒåœ¨æ•°æ®é›†ä¸Šè·å¾—çš„ç›¸åº”çš„æƒŠäººæ€§èƒ½ã€‚ç¬¬ä¸€ä¸ªç½‘ç»œå«åš Alexnetã€‚

ç„¶åï¼Œè°·æ­Œçš„ Inception networks å¼•å…¥äº†ä¸€ç§æ–°çš„æ¦‚å¿µï¼Œå³é‡‡ç”¨ä¸åŒå¤§å°çš„è¿‡æ»¤å™¨æ¥æ¥æ”¶ç›¸åŒçš„è¾“å…¥ï¼Œå¹¶åœ¨ Imagenet æŒ‘æˆ˜ä¸­æˆåŠŸè¶…è¶Šäº† Alexnetã€‚ç„¶åæ˜¯ Resnetã€VGG-16 å’Œè®¸å¤šå…¶ä»–ç®—æ³•ï¼Œå®ƒä»¬åœ¨ Imagenet å’Œ Alexnet çš„åŸºç¡€ä¸Šç»§ç»­æ”¹è¿›ã€‚

ä¸æ­¤åŒæ—¶ï¼ŒNLP é¢†åŸŸä¹Ÿåœ¨è¿…é€Ÿå‘å±•ï¼Œéšç€å¼€åˆ›æ€§è®ºæ–‡[çš„å‘å¸ƒï¼Œä½ æ‰€éœ€è¦çš„å°±æ˜¯å…³æ³¨ï¼](https://arxiv.org/abs/1706.03762)ï¼Œä»æ¥æ²¡æœ‰ä¸€æ ·ã€‚è™½ç„¶æ³¨æ„åŠ›ä»æ ¹æœ¬ä¸Šæ”¹å˜äº† NLPï¼Œä½†å®ƒä»ç„¶æ²¡æœ‰å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰é—®é¢˜ã€‚

åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬è¯•ç€æ›´å¥½åœ°ç†è§£æ³¨æ„åŠ›ã€‚

ç¥ç»ç½‘ç»œä¸­çš„æ³¨æ„æœºåˆ¶å€¾å‘äºæ¨¡ä»¿äººç±»æ‰€æ‹¥æœ‰çš„è®¤çŸ¥æ³¨æ„ã€‚è¯¥åŠŸèƒ½çš„ä¸»è¦ç›®çš„æ˜¯å¼ºè°ƒä¿¡æ¯çš„é‡è¦éƒ¨åˆ†ï¼Œå¹¶å°½é‡ä¸å¼ºè°ƒä¸ç›¸å…³çš„éƒ¨åˆ†ã€‚ç”±äºäººç±»å’Œæœºå™¨çš„å·¥ä½œè®°å¿†éƒ½æ˜¯æœ‰é™çš„ï¼Œæ‰€ä»¥è¿™ä¸ªè¿‡ç¨‹æ˜¯é¿å…ç³»ç»Ÿè®°å¿†è´Ÿæ‹…è¿‡é‡çš„å…³é”®ã€‚åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ³¨æ„åŠ›å¯ä»¥è¢«è§£é‡Šä¸ºé‡è¦æ€§æƒé‡çš„å‘é‡ã€‚å½“æˆ‘ä»¬é¢„æµ‹ä¸€ä¸ªå…ƒç´ æ—¶ï¼Œå®ƒå¯èƒ½æ˜¯å›¾åƒä¸­çš„ä¸€ä¸ªåƒç´ æˆ–å¥å­ä¸­çš„ä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„åŠ›å‘é‡æ¥æ¨æ–­å®ƒä¸å…¶ä»–å…ƒç´ çš„ç›¸å…³ç¨‹åº¦ã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ç¯‡ä¼˜ç§€åšå®¢ä¸­æåˆ°çš„ä¸€ä¸ªä¾‹å­ï¼Œ *[å…³æ³¨ï¼Ÿç«‹æ­£ï¼](https://lilianweng.github.io/posts/2018-06-24-attention/)* ã€‚

![](img/608497e73c057320bdda962b2d8d0fb7.png)

The name of the dog is Shiba Inu

å¦‚æœæˆ‘ä»¬æŠŠæ³¨æ„åŠ›é›†ä¸­åœ¨çº¢è‰²ç›’å­é‡Œçš„ç‹—çš„ç‰¹å¾ä¸Šï¼Œæ¯”å¦‚å®ƒçš„é¼»å­ã€å°–å°–çš„å³è€³å’Œç¥ç§˜çš„çœ¼ç›ï¼Œæˆ‘ä»¬å°±èƒ½çŒœå‡ºé»„è‰²ç›’å­é‡Œåº”è¯¥æ˜¯ä»€ä¹ˆã€‚ç„¶è€Œï¼Œä»…é€šè¿‡æŸ¥çœ‹ç°è‰²æ¡†ä¸­çš„åƒç´ ï¼Œæ‚¨å°†æ— æ³•é¢„æµ‹é»„è‰²æ¡†ä¸­åº”è¯¥å‡ºç°ä»€ä¹ˆã€‚æ³¨æ„æœºåˆ¶å¯¹æ­£ç¡®æ–¹æ¡†ä¸­çš„åƒç´ æ¯”å¯¹é»„è‰²æ–¹æ¡†ä¸­çš„åƒç´ çš„æƒé‡æ›´å¤§ã€‚è€Œç°è‰²æ¡†ä¸­çš„åƒç´ æƒé‡è¾ƒä½ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å¯¹æ³¨æ„åŠ›çš„ä½œç”¨æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„äº†è§£ã€‚åœ¨è¿™ä¸ªåšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›ç”¨äºä¸€ä¸ªå´‡é«˜çš„è¿½æ±‚ã€‚æˆ‘ä»¬å°†é»‘è‰²ç´ ç˜¤å›¾åƒåˆ†ä¸ºæ¶æ€§å’Œè‰¯æ€§ï¼Œç„¶åå°è¯•è§£é‡Šè¿™äº›é¢„æµ‹ã€‚

ä½†æ˜¯æˆ‘ä»¬å¯ä»¥åœ¨æ²¡æœ‰æ³¨æ„çš„æƒ…å†µä¸‹å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¯¹å—ï¼Ÿ

å¦‚æœä½ ä»ç„¶ä¸ç›¸ä¿¡ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥æŠŠæ³¨æ„åŠ›æ”¾åœ¨å›¾åƒä¸Šï¼Œè¿™é‡Œæœ‰æ›´å¤šçš„è§‚ç‚¹æ¥è¯æ˜è¿™ä¸€ç‚¹:

*   åœ¨è®­ç»ƒå›¾åƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿå…³æ³¨å›¾åƒçš„é‡è¦éƒ¨åˆ†ã€‚å®ç°è¿™ä¸€ç‚¹çš„æ–¹æ³•ä¹‹ä¸€æ˜¯é€šè¿‡ ****å¯è®­ç»ƒçš„æ³¨æ„åŠ›**** æœºåˆ¶(ä½†ä½ å·²ç»çŸ¥é“è¿™ä¸€ç‚¹äº†ï¼Œå¯¹å—ï¼Ÿç»§ç»­è¯»..)
*   åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨å¤„ç†ç—…å˜å›¾åƒï¼Œå¹¶ä¸”èƒ½å¤Ÿ ****è§£é‡Š**** æ¨¡å‹å˜å¾—æ›´åŠ å¿…è¦ã€‚ç†è§£å›¾åƒçš„å“ªä¸€éƒ¨åˆ†å¯¹ç™Œç—‡è¢«åˆ†ç±»ä¸ºè‰¯æ€§/æ¶æ€§æ›´æœ‰è´¡çŒ®æ˜¯å¾ˆé‡è¦çš„ã€‚
*   åƒ ****Grad-CAM**** è¿™æ ·çš„äº‹ååˆ†æå¹¶ä¸ç­‰åŒäºå…³æ³¨ã€‚å®ƒä»¬å¹¶ä¸æ‰“ç®—æ”¹å˜æ¨¡å‹å­¦ä¹ çš„æ–¹å¼ï¼Œæˆ–è€…æ”¹å˜æ¨¡å‹å­¦ä¹ çš„å†…å®¹ã€‚å®ƒä»¬è¢«åº”ç”¨äºå·²ç»è®­ç»ƒå¥½çš„å…·æœ‰å›ºå®šæƒé‡çš„æ¨¡å‹ï¼Œå¹¶ä¸”ä»…ç”¨äºæä¾›å¯¹æ¨¡å‹å†³ç­–çš„æ´å¯Ÿã€‚

ç°åœ¨ï¼Œä½ å·²ç»å¯¹æˆ‘ä»¬ä¸ºä»€ä¹ˆä½¿ç”¨æ³¨æ„åŠ›è¿›è¡Œå›¾åƒåˆ†ç±»æœ‰äº†å…¨é¢çš„äº†è§£ï¼Œè®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€ä¸‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Pytorchã€‚å®ƒæ›´å†—é•¿ï¼Œä¼¼ä¹æœ‰å¾ˆå¤šä»£ç ï¼Œä½†ç”±äºå®ƒå¹¿æ³›ä½¿ç”¨äº†ç±»ï¼Œæ‰€ä»¥æ›´ pythonic åŒ–ï¼Œå¹¶ä¸”ä¸ TensorFlow ç›¸æ¯”ï¼Œç»™äº†ç”¨æˆ·æ›´å¤šçš„æ§åˆ¶æƒã€‚

æˆ‘æ­£åœ¨ä½¿ç”¨æ¥è‡ª Kaggle çš„è¿™ä¸ªæ•°æ®é›†ã€‚æ‰€æœ‰å›¾åƒéƒ½æ˜¯çš®è‚¤æŸä¼¤ï¼Œå½¢çŠ¶ä¸º 512 x 512 x 3ã€‚è¦è®¾ç½® Kaggle API å¹¶å°†æ•°æ®é›†ä¸‹è½½åˆ°æ¸å˜ç¬”è®°æœ¬ä¸­ä»¥ä¸‹è½½è¯¥æ•°æ®ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œ:

*   é¦–å…ˆï¼Œåˆ›å»ºå¹¶ç™»å½• Kaggle å¸æˆ·
*   å…¶æ¬¡ï¼Œé€šè¿‡è¿›å…¥æ‚¨çš„å¸æˆ·è®¾ç½®åˆ›å»ºä¸€ä¸ª API ä»¤ç‰Œï¼Œå¹¶å°† kaggle.json ä¿å­˜åˆ°æ‚¨çš„æœ¬åœ°æœºå™¨ä¸Š
*   ä¸‰ã€ä¸Šä¼  kaggle.json åˆ°æ¸å˜è®°äº‹æœ¬ç¬¬å››ã€æŠŠæ–‡ä»¶ç§»åˆ°~/ã€‚kaggle/ä½¿ç”¨ç»ˆç«¯å‘½ä»¤`cp kaggle.json ~/.kaggle/`
*   å››ã€å®‰è£… kaggle: `pip install kaggle`
*   ç¬¬äº”ï¼Œä½¿ç”¨ API é€šè¿‡ç»ˆç«¯ä¸‹è½½æ•°æ®é›†:`kaggle datasets download shonenkov/melanoma-merged-external-data-512x512-jpeg`
*   å…­ã€ä½¿ç”¨ç»ˆç«¯è§£å‹æ•°æ®é›†:`unzip melanoma-merged-external-data-512x512-jpeg.zip`

æ‚¨è¿˜éœ€è¦å†åšå‡ ä¸ªæ­¥éª¤æ¥è®¾ç½®ä»¥ä½¿å…¶æ­£ç¡®è¿è¡Œã€‚åœ¨ç»ˆç«¯ä¸­ï¼Œ`pip install opencv-python kaggle`ç„¶åè¿è¡Œ`apt-get install libgl1`ã€‚

ç„¶åé€šè¿‡è¿è¡ŒåŒ…å«ä»¥ä¸‹å†…å®¹çš„å•å…ƒæ¥å¯¼å…¥ä»¥ä¸‹åº“:

```py
import pandas as pd

from sklearn.model_selection import train_test_split
from torchvision import transforms
import torch.nn as nn

import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
import torchvision.models as models

from torch.utils.data import DataLoader 
```

## é¢„å¤„ç†æ•°æ®

æˆ‘åªä½¿ç”¨äº†å›¾åƒä¸­çš„ä¸€ä¸ªæ ·æœ¬(å› ä¸ºæ•°æ®é›†å¾ˆå¤§),å¹¶åšäº†ä»¥ä¸‹é¢„å¤„ç†:

*   è°ƒæ•´å¤§å°ã€æ ‡å‡†åŒ–ã€å±…ä¸­å’Œè£å‰ªè®­ç»ƒå’Œæµ‹è¯•å›¾åƒã€‚
*   ä»…åœ¨è®­ç»ƒå›¾åƒä¸Šçš„æ•°æ®æ‰©å……(éšæœºæ—‹è½¬/æ°´å¹³ç¿»è½¬)ã€‚

```py
# read the data 
data_dir='melanoma-merged-external-data-512x512-jpeg/512x512-dataset-melanoma/512x512-dataset-melanoma/'
data=pd.read_csv('melanoma-merged-external-data-512x512-jpeg/marking.csv')

# balance the data a bit
df_0=data[data['target']==0].sample(6000,random_state=42)
df_1=data[data['target']==1]
data=pd.concat([df_0,df_1]).reset_index()

#prepare the data
labels=[]
images=[]
for i in range(data.shape[0]):
    images.append(data_dir + data['image_id'].iloc[i]+'.jpg')
    labels.append(data['target'].iloc[i])
df=pd.DataFrame(images)
df.columns=['images']
df['target']=labels

# Split train into train and val
X_train, X_val, y_train, y_val = train_test_split(df['images'],df['target'], test_size=0.2, random_state=1234) 
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch çš„è½¬æ¢æ¨¡å—æ¥è½¬æ¢å›¾åƒã€‚

```py
train_transform = transforms.Compose([
        transforms.RandomRotation(10),      # rotate +/- 10 degrees
        transforms.RandomHorizontalFlip(),  # reverse 50% of images
        transforms.Resize(224),             # resize shortest side to 224 pixels
        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])

val_transform = transforms.Compose([
        transforms.Resize(224),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])
```

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªç»§æ‰¿ PyTorch çš„ Dataset ç±»çš„ç±»ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªç±»æ¥è¯»å–ã€è½¬æ¢å’Œæ”¾å¤§å›¾åƒã€‚

```py
class ImageDataset(Dataset):

    def __init__(self,data_paths,labels,transform=None,mode='train'):
         self.data=data_paths
         self.labels=labels
         self.transform=transform
         self.mode=mode
    def __len__(self):
       return len(self.data)

    def __getitem__(self,idx):
        img_name = self.data[idx]
        img = cv2.imread(img_name)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img=Image.fromarray(img)
        if self.transform is not None:
          img = self.transform(img)
        img=img.cuda()

        labels = torch.tensor(self.labels[idx]).cuda()

        return img, labels

train_dataset=ImageDataset(data_paths=X_train.values,labels=y_train.values,transform=train_transform)
val_dataset=ImageDataset(data_paths=X_val.values,labels=y_val.values,transform=val_transform)

train_loader=DataLoader(train_dataset,batch_size=100,shuffle=True)
val_loader=DataLoader(val_dataset,batch_size=50,shuffle=False)
```

### å‹å·- VGG16 æ³¨æ„

åœ¨å®é™…çš„åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¸¦æœ‰æ³¨æ„åŠ›å±‚çš„ VGG16ã€‚è¯¥æ¶æ„æœ€åˆæ˜¯åœ¨è¿™ç¯‡è®ºæ–‡ä¸­æå‡ºçš„ï¼Œä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„åšå®¢[è¿™é‡Œ](https://blog.paperspace.com/vgg-from-scratch-pytorch/)æ‰¾åˆ°ä»å¤´å¼€å§‹ç¼–å†™è¯¥ç®—æ³•çš„æŒ‡å—(è¾ƒå°‘å…³æ³¨æ³¨æ„åŠ›æœºåˆ¶)ã€‚ç»™ä½ ä¸€ä¸ª VGG16 çš„å…¥é—¨ï¼Œå®ƒæœ‰ 16 å±‚æ·±ï¼Œè®¾è®¡åœ¨ 2014 å¹´èµ¢å¾—äº† ImageNet ç«èµ›ã€‚ä»–ä»¬ä½¿ç”¨ 3Ã—3 æ»¤æ³¢å™¨å¤§å°çš„å·ç§¯å±‚ï¼Œæ­¥é•¿ä¸º 1ï¼ŒReLu ä½œä¸ºå…¶æ¿€æ´»å‡½æ•°ã€‚Maxpooling å±‚å…·æœ‰è·¨è·ä¸º 2 çš„ 2x2 è¿‡æ»¤å™¨ã€‚æœ€åæœ‰ 2 ä¸ªå¯†é›†å±‚ï¼Œåé¢æ˜¯ä¸€ä¸ª softmax å±‚ã€‚

VGG16 å°†æ˜¯ä¸»å¹²ï¼Œä¸ä¼šæœ‰ä»»ä½•å¯†é›†å±‚ã€‚

![](img/5684046f13ca6068ae187285b309a200.png)

[[Source](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf)]

*   åº”ç”¨äº†ä¸¤ä¸ªæ³¨æ„æ¨¡å—(ç°è‰²å—)ã€‚ä¸­é—´ç‰¹å¾å›¾(æ±  3 å’Œæ±  4)çš„è¾“å‡ºç”¨äºæ¨æ–­æ³¨æ„åŠ›å›¾ã€‚pool-5 çš„è¾“å‡ºå……å½“ä¸€ç§å½¢å¼çš„å…¨å±€å¼•å¯¼ï¼Œå› ä¸ºæœ€åä¸€çº§ç‰¹å¾åŒ…å«æ•´ä¸ªå›¾åƒçš„æœ€æŠ½è±¡å’Œæœ€å‹ç¼©çš„ä¿¡æ¯ã€‚
*   ä¸‰ä¸ªç‰¹å¾å‘é‡(ç»¿è‰²å—)é€šè¿‡å…¨å±€å¹³å‡æ± è®¡ç®—ï¼Œå¹¶è¿æ¥åœ¨ä¸€èµ·ä»¥å½¢æˆæœ€ç»ˆçš„ç‰¹å¾å‘é‡ï¼Œå…¶ç”¨ä½œåˆ†ç±»å±‚çš„è¾“å…¥(æ­¤å¤„æœªç¤ºå‡º)ã€‚

å¦‚æœæ‚¨å¯¹æ­¤ä¸å¤ªæ¸…æ¥šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œæˆ‘å°†åœ¨ä¸‹ä¸€æ­¥å¯¹å…¶è¿›è¡Œè¯¦ç»†è¯´æ˜ã€‚

### å®ç°å…³æ³¨å±‚

ä¸‹é¢æ˜¯ä¸€ä¸ªå®šä¹‰æ³¨æ„åŠ›å±‚çš„ç±»ã€‚

```py
class AttentionBlock(nn.Module):
    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):
        super(AttentionBlock, self).__init__()
        self.up_factor = up_factor
        self.normalize_attn = normalize_attn
        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=1, padding=0, bias=False)
        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=1, padding=0, bias=False)
        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)
    def forward(self, l, g):
        N, C, W, H = l.size()
        l_ = self.W_l(l)
        g_ = self.W_g(g)
        if self.up_factor > 1:
            g_ = F.interpolate(g_, scale_factor=self.up_factor, mode='bilinear', align_corners=False)
        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH

        # compute attn map
        if self.normalize_attn:
            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)
        else:
            a = torch.sigmoid(c)
        # re-weight the local feature
        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH
        if self.normalize_attn:
            output = f.view(N,C,-1).sum(dim=2) # weighted sum
        else:
            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling
        return a, output 
```

è¿™ä¸ªå›¾å¯ä»¥è§£é‡Šæ³¨æ„åŠ›å±‚å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆã€‚

![](img/25ec2d62f82543ac5fe7e454b4009e3f.png)

[Source]

*   ä¸­é—´ç‰¹å¾å‘é‡(F)æ˜¯æ± -3 æˆ–æ± -4 çš„è¾“å‡ºï¼Œè€Œå…¨å±€ç‰¹å¾å‘é‡(æ± -5 çš„è¾“å‡º)ä½œä¸ºè¾“å…¥è¢«é¦ˆé€åˆ°æ³¨æ„å±‚ã€‚
*   ä¸¤ä¸ªç‰¹å¾å‘é‡éƒ½é€šè¿‡å·ç§¯å±‚ã€‚å½“å…¨å±€ç‰¹å¾å’Œä¸­é—´ç‰¹å¾çš„ç©ºé—´å¤§å°ä¸åŒæ—¶ï¼Œé€šè¿‡åŒçº¿æ€§æ’å€¼è¿›è¡Œç‰¹å¾ä¸Šé‡‡æ ·ã€‚ *up_factor* ç¡®å®šå·ç§¯çš„å…¨å±€ç‰¹å¾å‘é‡å¿…é¡»è¢«æ”¾å¤§çš„å› å­ã€‚
*   ä¹‹åï¼Œè¿›è¡Œé€å…ƒç´ æ±‚å’Œï¼Œç„¶åè¿›è¡Œå·ç§¯è¿ç®—ï¼Œå°† 256 ä¸ªé€šé“å‡å°‘åˆ° 1 ä¸ªã€‚
*   è¿™ç„¶åè¢«è¾“å…¥åˆ°ä¸€ä¸ª Softmax å±‚ï¼Œè¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ³¨æ„åŠ›åœ°å›¾(A)ã€‚A ä¸­çš„æ¯ä¸ªæ ‡é‡å…ƒç´ ä»£è¡¨ f ä¸­å¯¹åº”çš„ç©ºé—´ç‰¹å¾å‘é‡çš„å—å…³æ³¨ç¨‹åº¦ã€‚
*   ç„¶åé€šè¿‡*é€åƒç´ *ä¹˜æ³•è®¡ç®—æ–°çš„ç‰¹å¾å‘é‡ğ¹Ì‚ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªç‰¹å¾å‘é‡ f ä¹˜ä»¥å…³æ³¨å…ƒç´  a
*   å› æ­¤ï¼Œæ³¨æ„åŠ›å›¾ a å’Œæ–°çš„ç‰¹å¾å‘é‡ğ¹Ì‚æ˜¯æ³¨æ„åŠ›å±‚çš„è¾“å‡ºã€‚

```py
class AttnVGG(nn.Module):
    def __init__(self, num_classes, normalize_attn=False, dropout=None):
        super(AttnVGG, self).__init__()
        net = models.vgg16_bn(pretrained=True)
        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])
        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])
        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])
        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])
        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])
        self.pool = nn.AvgPool2d(7, stride=1)
        self.dpt = None
        if dropout is not None:
            self.dpt = nn.Dropout(dropout)
        self.cls = nn.Linear(in_features=512+512+256, out_features=num_classes, bias=True)

       # initialize the attention blocks defined above
        self.attn1 = AttentionBlock(256, 512, 256, 4, normalize_attn=normalize_attn)
        self.attn2 = AttentionBlock(512, 512, 256, 2, normalize_attn=normalize_attn)

        self.reset_parameters(self.cls)
        self.reset_parameters(self.attn1)
        self.reset_parameters(self.attn2)
    def reset_parameters(self, module):
        for m in module.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1.)
                nn.init.constant_(m.bias, 0.)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0., 0.01)
                nn.init.constant_(m.bias, 0.)
    def forward(self, x):
        block1 = self.conv_block1(x)       # /1
        pool1 = F.max_pool2d(block1, 2, 2) # /2
        block2 = self.conv_block2(pool1)   # /2
        pool2 = F.max_pool2d(block2, 2, 2) # /4
        block3 = self.conv_block3(pool2)   # /4
        pool3 = F.max_pool2d(block3, 2, 2) # /8
        block4 = self.conv_block4(pool3)   # /8
        pool4 = F.max_pool2d(block4, 2, 2) # /16
        block5 = self.conv_block5(pool4)   # /16
        pool5 = F.max_pool2d(block5, 2, 2) # /32
        N, __, __, __ = pool5.size()

        g = self.pool(pool5).view(N,512)
        a1, g1 = self.attn1(pool3, pool5)
        a2, g2 = self.attn2(pool4, pool5)
        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C
        if self.dpt is not None:
            g_hat = self.dpt(g_hat)
        out = self.cls(g_hat)

        return [out, a1, a2]
```

*   VGG16 çš„æ¶æ„åŸºæœ¬ä¿æŒä¸å˜ï¼Œåªæ˜¯ç§»é™¤äº†å¯†é›†å±‚ã€‚
*   æˆ‘ä»¬å°†æ±  3 å’Œæ±  4 é€šè¿‡æ³¨æ„åŠ›å±‚ï¼Œå¾—åˆ°ğ¹Ì‚ 3 å’Œğ¹Ì‚ 4ã€‚
*   ğ¹Ì‚ 3ã€ğ¹Ì‚ 4 å’Œ G(pool-5)è¢«è¿æ¥èµ·æ¥å¹¶è¢«è¾“å…¥åˆ°æœ€ç»ˆçš„åˆ†ç±»å±‚ã€‚
*   æ•´ä¸ªç½‘ç»œè¢«ç«¯åˆ°ç«¯åœ°è®­ç»ƒã€‚

```py
model = AttnVGG(num_classes=1, normalize_attn=True)
model=model.cuda()
```

æˆ‘ä½¿ç”¨ç„¦ç‚¹æŸå¤±è€Œä¸æ˜¯å¸¸è§„çš„äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ•°æ®æ˜¯ä¸å¹³è¡¡çš„(åƒå¤§å¤šæ•°åŒ»å­¦æ•°æ®é›†ä¸€æ ·)ï¼Œå¹¶ä¸”ç„¦ç‚¹æŸå¤±å¯ä»¥è‡ªåŠ¨é™ä½è®­ç»ƒé›†ä¸­æ ·æœ¬çš„æƒé‡ã€‚

```py
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, logits=False, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits
        self.reduce = reduce

    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss

criterion = FocalLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
```

ç°åœ¨è¯¥è®­ç»ƒæ¨¡å‹äº†ã€‚æˆ‘ä»¬å°†å¯¹ 2 ä¸ªæ—¶æœŸæ‰§è¡Œæ­¤æ“ä½œï¼Œæ‚¨å¯ä»¥é€šè¿‡æ›´æ”¹åˆ†é…ç»™ä¸‹è¿°å•å…ƒæ ¼ä¸­â€œæ—¶æœŸâ€å˜é‡çš„å€¼æ¥æŸ¥çœ‹å¢åŠ æ—¶æœŸæ˜¯å¦ä¼šæé«˜æ€§èƒ½ã€‚

```py
import time
start_time = time.time()

epochs = 2

train_losses = []
train_auc=[]
val_auc=[]

for i in range(epochs):

    train_preds=[]
    train_targets=[]
    auc_train=[]
    loss_epoch_train=[]
    # Run the training batches
    for b, (X_train, y_train) in tqdm(enumerate(train_loader),total=len(train_loader)):

        b+=1
        y_pred,_,_=model(X_train)
        loss = criterion(torch.sigmoid(y_pred.type(torch.FloatTensor)), y_train.type(torch.FloatTensor))   
        loss_epoch_train.append(loss.item())
        # For plotting purpose
        if (i==1):
            if (b==19):
                I_train = utils.make_grid(X_train[0:8,:,:,:], nrow=8, normalize=True, scale_each=True)
                __, a1, a2 = model(X_train[0:8,:,:,:])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    try:
        auc_train=roc_auc_score(y_train.detach().to(device).numpy(),torch.sigmoid(y_pred).detach().to(device).numpy())
    except:
        auc_train=0
    train_losses.append(np.mean(loss_epoch_train))
    train_auc.append(auc_train)
    print(f'epoch: {i:2}   loss: {np.mean(loss_epoch_train):10.8f} AUC  : {auc_train:10.8f} ')
    # Run the testing batches

    with torch.no_grad():
        for b, (X_test, y_test) in enumerate(val_loader):

            y_val,_,_ = model(X_test)
            loss = criterion(torch.sigmoid(y_val.type(torch.FloatTensor)), y_test.type(torch.FloatTensor))
            loss_epoch_test.append(loss.item())
    val_auc.append(auc_val)
    print(f'Epoch: {i} Val Loss: {np.mean(loss_epoch_test):10.8f} AUC: {auc_val:10.8f} ')

print(f'\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed
```

æˆ‘ä»¬çš„è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤º:

![](img/e1eb200972fd7c86e8a90848e71fc918.png)

éªŒè¯ AUC çœ‹èµ·æ¥ä¸é”™ï¼Œç°åœ¨è®©æˆ‘ä»¬è§£é‡Šæ¨¡å‹ã€‚

## è§†è§‰åŒ–æ³¨æ„åŠ›

æˆ‘ä»¬å°†å¯è§†åŒ–ç”± pool-3 å’Œ pool-4 åˆ›å»ºçš„æ³¨æ„åŠ›åœ°å›¾ï¼Œä»¥äº†è§£å›¾åƒçš„å“ªä¸€éƒ¨åˆ†è´Ÿè´£åˆ†ç±»ã€‚

```py
def visualize_attention(I_train,a,up_factor,no_attention=False):
    img = I_train.permute((1,2,0)).cpu().numpy()
    # compute the heatmap
    if up_factor > 1:
        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)
    attn = utils.make_grid(a, nrow=8, normalize=True, scale_each=True)
    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()
    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)
    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)
    attn = np.float32(attn) / 255
    # add the heatmap to the image
    img=cv2.resize(img,(466,60))
    if no_attention:
        return torch.from_numpy(img)
    else:
        vis = 0.6 * img + 0.4 * attn
        return torch.from_numpy(vis) 
```

```py
orig=visualize_attention(I_train,a1,up_factor=2,no_attention=True)
first=visualize_attention(I_train,a1,up_factor=2,no_attention=False)
second=visualize_attention(I_train,a2,up_factor=4,no_attention=False)

fig, (ax1, ax2,ax3) = plt.subplots(3, 1,figsize=(10, 10))
ax1.imshow(orig)
ax2.imshow(first)
ax3.imshow(second)
ax1.title.set_text('Input Images')
ax2.title.set_text('pool-3 attention')
ax3.title.set_text('pool-4 attention')
```

![](img/e57f7870f907ae7e8405c222736bf0c2.png)

*   ****æ¶æ€§å›¾åƒçš„å·¥ä½œæ–¹å¼**** :-è¾ƒæµ…çš„å±‚(pool-3)å€¾å‘äºèšç„¦äºæ›´ä¸€èˆ¬å’Œæ‰©æ•£çš„åŒºåŸŸï¼Œè€Œè¾ƒæ·±çš„å±‚(pool-4)åˆ™æ›´é›†ä¸­ï¼Œèšç„¦äºç—…ç¶ï¼Œé¿å¼€æ— å…³åƒç´ ã€‚
*   ä½†ç”±äºæˆ‘ä»¬çš„æƒ…å†µä¸‹å¤§å¤šæ•°å›¾åƒéƒ½æ˜¯è‰¯æ€§çš„ï¼Œpool-3 è¯•å›¾å­¦ä¹ ä¸€äº›åŒºåŸŸï¼Œä½† pool-4 æœ€ç»ˆä¼šæœ€å°åŒ–æ¿€æ´»çš„åŒºåŸŸï¼Œå› ä¸ºå›¾åƒæ˜¯è‰¯æ€§çš„ã€‚

### ç»“è®º

è¿™ç¯‡åšæ–‡åŸºæœ¬å±•ç¤ºäº†å¦‚ä½•å°†æ³¨æ„åŠ›æœºåˆ¶ä¸é¢„å…ˆè®­ç»ƒå¥½çš„å›¾åƒæ¨¡å‹ç»“åˆä½¿ç”¨ï¼Œå¹¶æ­ç¤ºäº†ä½¿ç”¨å®ƒçš„å¥½å¤„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸè®ºæ–‡è¿˜å£°ç§°ï¼Œç”±äºæ¶ˆé™¤äº†å¯†é›†å±‚ï¼Œå‚æ•°æ•°é‡å¤§å¤§å‡å°‘ï¼Œç½‘ç»œè®­ç»ƒæ›´è½»ã€‚å¦‚æœä½ æ‰“ç®—åœ¨ä»Šåçš„å·¥ä½œä¸­åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¯·è®°ä½è¿™ä¸€ç‚¹ã€‚å¦‚æœä½ æƒ³è¿›ä¸€æ­¥æé«˜å›¾åƒåˆ†ç±»æŠ€æœ¯ï¼Œè¿˜æœ‰ä¸€äº›äº‹æƒ…ä½ å¯ä»¥è‡ªå·±å°è¯•ï¼Œå¦‚è°ƒæ•´è¶…å‚æ•°ï¼Œæ”¹å˜ä¸»å¹²ç»“æ„æˆ–ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚

### å‚è€ƒ

*   [https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf](https://www2.cs.sfu.ca/~hamarneh/ecopy/ipmi2019.pdf)
*   [https://towards data science . com/learn-to-pay-attention-training-visual-attention-in-CNNs-87e 2869 f89 f1](https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1)
*   [https://github . com/SaoYan/IPMI 2019-Attn Mel/tree/99 E4 a9 b 71717 FB 51 f 24d 7994948 b 6 a 0 e 76 bb 8d 58](https://github.com/SaoYan/IPMI2019-AttnMel/tree/99e4a9b71717fb51f24d7994948b6a0e76bb8d58)