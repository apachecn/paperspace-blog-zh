# å˜å‹å™¨çš„ç›´è§‚ä»‹ç»

> åŸæ–‡ï¼š<https://blog.paperspace.com/attention-is-all-you-need-the-components-of-the-transformer/>

### ä»‹ç»

è¿™ç¯‡[è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)æ˜¯åœ¨åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶æ–¹é¢å‘å‰è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ï¼Œæ˜¯ä¸€ä¸ªè¢«ç§°ä¸º transformer çš„æ¨¡å‹çš„ä¸»è¦å‘å±•ã€‚ç›®å‰ä¸º NLP ä»»åŠ¡å¼€å‘çš„æœ€è‘—åçš„æ¨¡å‹ç”±è®¸å¤šå˜å‹å™¨ç»„æˆã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ¦‚è¿°è¿™ä¸ªæ¨¡å‹çš„ç»„ä»¶ã€‚

### æˆ‘ä»¬éœ€è¦å˜å‹å™¨çš„åŸå› 

åœ¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„ä¸­ä½¿ç”¨ RNNs çš„ç¬¬ä¸€ä»£ç¥ç»æœºå™¨ç¿»è¯‘æ–¹æ³•é›†ä¸­äºä½¿ç”¨ RNNs æ¥åº”å¯¹åºåˆ—é—´çš„æŒ‘æˆ˜ã€‚

å½“å¤„ç†æ‰©å±•åºåˆ—æ—¶ï¼Œè¿™äº›è®¾è®¡æœ‰ä¸€ä¸ªæ˜æ˜¾çš„ç¼ºç‚¹ï¼›å½“é¢å¤–çš„å…ƒç´ è¢«å¼•å…¥åºåˆ—æ—¶ï¼Œå®ƒä»¬ä¿å­˜æœ€åˆæˆåˆ†ä¿¡æ¯çš„èƒ½åŠ›å°±ä¸§å¤±äº†ã€‚ç¼–ç å™¨ä¸­çš„æ¯ä¸€æ­¥éƒ½æœ‰ä¸€ä¸ªéšè—çŠ¶æ€ï¼Œé€šå¸¸ä¸æœ€è¿‘è¾“å…¥çš„ä¸€ä¸ªå•è¯ç›¸å…³è”ã€‚å› æ­¤ï¼Œå¦‚æœè§£ç å™¨åªè®¿é—®ç¼–ç å™¨çš„æœ€æ–°éšè—çŠ¶æ€ï¼Œå®ƒå°†ä¸¢å¤±å…³äºåºåˆ—çš„æ—©æœŸåˆ†é‡çš„ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°æ¦‚å¿µ:æ³¨æ„æœºåˆ¶ã€‚

è™½ç„¶ rnn é€šå¸¸å…³æ³¨ç¼–ç å™¨çš„æœ€åçŠ¶æ€ï¼Œä½†æˆ‘ä»¬åœ¨è§£ç è¿‡ç¨‹ä¸­ä¼šæŸ¥çœ‹ç¼–ç å™¨çš„æ‰€æœ‰çŠ¶æ€ï¼Œä»è€Œè·å¾—è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªå…ƒç´ çš„ä¿¡æ¯ã€‚è¿™å°±æ˜¯ attention æ‰€åšçš„:å®ƒä»æ•´ä¸ªåºåˆ—ä¸­ç¼–è¯‘æ‰€æœ‰å…ˆå‰ç¼–ç å™¨çŠ¶æ€çš„åŠ æƒå’Œã€‚

å› æ­¤ï¼Œè§£ç å™¨å¯èƒ½ä¼šä¸ºè¾“å‡ºçš„æ¯ä¸ªå…ƒç´ åŒºåˆ†ç‰¹å®šè¾“å…¥å…ƒç´ çš„ä¼˜å…ˆçº§ã€‚é€šè¿‡åœ¨æ¯ä¸ªé˜¶æ®µå…³æ³¨æ­£ç¡®çš„è¾“å…¥å…ƒç´ æ¥å­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºå…ƒç´ ã€‚

ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„åˆ°è¿™ç§ç­–ç•¥ä»ç„¶æœ‰ä¸€ä¸ªæ˜æ˜¾çš„é™åˆ¶:æ¯ä¸ªåºåˆ—å¿…é¡»å•ç‹¬å¤„ç†ã€‚ä¸ºäº†å¤„ç†ç¬¬ t æ­¥ï¼Œç¼–ç å™¨å’Œè§£ç å™¨éƒ½å¿…é¡»ç­‰å¾…å‰é¢çš„ t-1 æ­¥å®Œæˆã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œå¤„ç†å¤§å‹æ•°æ®é›†éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå¹¶ä½¿ç”¨å¤§é‡è®¡ç®—èƒ½åŠ›ã€‚

## å‡ ç§ç±»å‹çš„å…³æ³¨

### è½¯ç¡¬æ³¨æ„

æ­£å¦‚ Luong ç­‰äººåœ¨ä»–ä»¬çš„æ–‡ç« ä¸­æåˆ°çš„ä»¥åŠå’Œå¾ç­‰äººåœ¨ä»–ä»¬çš„è®ºæ–‡ä¸­è¯¦è¿°çš„ï¼Œå½“æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å‘é‡è®¡ç®—ä¸ºç¼–ç å™¨éšè—çŠ¶æ€çš„åŠ æƒå’Œæ—¶ï¼Œä¼šå‘ç”Ÿè½¯æ³¨æ„ã€‚å½“æˆ‘ä»¬æ ¹æ®æ³¨æ„åŠ›åˆ†æ•°é€‰æ‹©ä¸€ä¸ªå•ç‹¬çš„éšè—çŠ¶æ€ï¼Œè€Œä¸æ˜¯æ‰€æœ‰éšè—çŠ¶æ€çš„åŠ æƒå¹³å‡å€¼æ—¶ï¼Œå°±ä¼šäº§ç”Ÿç¡¬æ³¨æ„åŠ›ã€‚é€‰æ‹©æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åƒ argmax è¿™æ ·çš„å‡½æ•°ï¼Œä½†å› ä¸ºè¯¥å‡½æ•°ä¸å¯å¾®ï¼Œæ‰€ä»¥éœ€è¦æ›´é«˜çº§çš„è®­ç»ƒç­–ç•¥ã€‚

![](img/e154520bc41aed4819e1569afa0ba265.png)

[source](https://towardsdatascience.com/attention-in-neural-networks-e66920838742)

### å…¨çƒå…³æ³¨

æœ¯è¯­â€œå…¨å±€æ³¨æ„åŠ›â€æŒ‡çš„æ˜¯æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€è¢«ç”¨äºä¸ºæ¯ä¸ªè§£ç å™¨æ­¥éª¤æ„å»ºåŸºäºæ³¨æ„åŠ›çš„ä¸Šä¸‹æ–‡å‘é‡çš„æƒ…å†µã€‚

æ­£å¦‚ä½ å¯èƒ½å·²ç»æƒ³åˆ°çš„ï¼Œè¿™å¯èƒ½ä¼šå¾ˆå¿«å˜å¾—æ˜‚è´µã€‚å½“å¯¼å‡ºä¸Šä¸‹æ–‡å‘é‡æ—¶ï¼Œå…¨å±€æ³¨æ„åŠ›æ¨¡å‹è€ƒè™‘ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€( **ct** )ã€‚åœ¨è¯¥æ¨¡å‹ç±»å‹ä¸­ï¼Œé€šè¿‡å°†å½“å‰ç›®æ ‡éšè—çŠ¶æ€ **ht** ä¸æ¯ä¸ªæºéšè—çŠ¶æ€ **hs** è¿›è¡Œæ¯”è¾ƒï¼Œå¾—åˆ°å¤„çš„å¯å˜é•¿åº¦å¯¹é½å‘é‡**ï¼Œå…¶å¤§å°ç­‰äºæºä¾§çš„æ—¶é—´æ­¥é•¿æ•°ã€‚**

![](img/8c1c116fc4e6694ec451a891e6458d24.png)

[source](https://arxiv.org/pdf/1508.04025.pdf)

### å½“åœ°çš„å…³æ³¨

ä½¿ç”¨å…¨å±€æ³¨æ„çš„ç¼ºç‚¹æ˜¯ï¼Œå¯¹äºç›®æ ‡ç«¯çš„æ¯ä¸ªè¯ï¼Œå®ƒå¿…é¡»æ³¨æ„æºç«¯çš„æ‰€æœ‰è¯ã€‚è¿™ä¸ä»…è€—æ—¶è€—åŠ›ï¼Œè€Œä¸”æœ‰å¯èƒ½ä½¿ç¿»è¯‘è¾ƒé•¿çš„åºåˆ—(å¦‚æ®µè½æˆ–æ–‡æ¡£)å˜å¾—ä¸åˆ‡å®é™…ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨å±€éƒ¨æ³¨æ„æœºåˆ¶æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™ç§æœºåˆ¶å°†åªå…³æ³¨ä¸æ¯ä¸ªç›®æ ‡å•è¯ç›¸å…³è”çš„æºä½ç½®çš„æå°å­é›†ã€‚

[Xu et al.(2015)](http://proceedings.mlr.press/v37/xuc15.pdf) æè®®åœ¨è½¯ç¡¬æ³¨æ„æ¨¡å‹ä¹‹é—´çš„æƒè¡¡æˆä¸ºå¼€å‘è¯¥æ¨¡å‹ä»¥è§£å†³ç”Ÿæˆå›¾åƒå­—å¹•æŒ‘æˆ˜çš„åŠ¨åŠ›æ¥æºã€‚åœ¨ä»–ä»¬çš„ç ”ç©¶ä¸­ï¼Œâ€œè½¯æ³¨æ„â€æŒ‡çš„æ˜¯å…¨å±€æ³¨æ„æŠ€æœ¯ï¼Œå…¶ç‰¹ç‚¹æ˜¯åœ¨æºå›¾åƒçš„æ‰€æœ‰è¡¥ä¸ä¸Šâ€œæ¸©å’Œâ€åœ°åº”ç”¨æƒé‡ã€‚

å¦ä¸€æ–¹é¢ï¼Œç¡¬æ³¨æ„æ–¹æ³•ä¸€æ¬¡åªå…³æ³¨å›¾åƒçš„ä¸€ä¸ªéƒ¨åˆ†ã€‚ä¸ºäº†è®­ç»ƒä¸€ä¸ªç¡¬æ³¨æ„æ¨¡å‹ï¼Œäººä»¬å¿…é¡»ä½¿ç”¨æ›´å¤æ‚çš„æŠ€æœ¯ï¼Œå¦‚æ–¹å·®å‡å°‘æˆ–å¼ºåŒ–å­¦ä¹ ï¼Œå³ä½¿å®ƒä»¬åœ¨æ¨ç†æ—¶æ›´ä¾¿å®œã€‚æˆ‘ä»¬çš„å±€éƒ¨æ³¨æ„æœºåˆ¶é€‰æ‹©æ€§åœ°èšç„¦äºç‹­çª„çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶ä¸”æ˜¯å¯åŒºåˆ†çš„ã€‚è¿™ç§ç­–ç•¥çš„å¥½å¤„æ˜¯é¿å…äº†è½¯æ³¨æ„æ–¹æ³•ä¸­å¼•èµ·çš„æ˜‚è´µè®¡ç®—ï¼ŒåŒæ—¶ï¼Œå®ƒæ¯”ç¡¬æ³¨æ„æ–¹æ³•æ›´å®¹æ˜“è®­ç»ƒã€‚è¯¥æ¨¡å‹é¦–å…ˆåœ¨ç»™å®šæ—¶é—´ **t** ä¸ºæ¯ä¸ªç›®æ ‡å•è¯ç”Ÿæˆç”± **pt** è¡¨ç¤ºçš„å¯¹é½ä½ç½®ã€‚

æ­¤åï¼Œé€šè¿‡å¯¹çª—å£[pt Dï¼Œpt+D]å†…çš„æºéšè—çŠ¶æ€è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œç”Ÿæˆç”±**CT**è¡¨ç¤ºçš„ä¸Šä¸‹æ–‡å‘é‡ï¼›d æ˜¯å‡­ç»éªŒé€‰æ‹©çš„ã€‚ä¸å…¨å±€æ–¹æ³•ç›¸åï¼Œåœ¨å¤„çš„å±€éƒ¨å¯¹å‡†å‘é‡**ç°åœ¨å…·æœ‰å›ºå®šçš„ç»´åº¦ã€‚è¯¥æ¨¡å‹æœ‰ä¸¤ç§å¯èƒ½æ€§ï¼Œæ¦‚è¿°å¦‚ä¸‹:**

*   å•è°ƒå¯¹é½(local-m):æˆ‘ä»¬è®¾ **pt = t** å‡è®¾æºåºåˆ—å’Œç›®æ ‡åºåˆ—è¿‘ä¼¼å•è°ƒå¯¹é½ã€‚
*   é¢„æµ‹æ¯”å¯¹(local-p):æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹æ¯”å¯¹ä½ç½®ï¼Œè€Œä¸æ˜¯å‡è®¾å•è°ƒçš„æ¯”å¯¹ä½ç½®:

![](img/a59ffcf4ab837013064435e79d7fed68.png)

[source](https://arxiv.org/pdf/1508.04025.pdf)

æ¨¡å‹å‚æ•° **wp** å’Œ **vp** å°†è¢«å­¦ä¹ ä»¥è¿›è¡Œä½ç½®é¢„æµ‹ã€‚
æºå¥å­çš„é•¿åº¦ç”±â€œSâ€è¡¨ç¤ºã€‚ **pt** è½åœ¨èŒƒå›´**ã€0ï¼ŒSã€‘**å†…ï¼Œè¿™æ˜¯ sigmoid å‡½æ•°çš„ç›´æ¥ç»“æœã€‚æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªä»¥**ç‚¹**ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒï¼Œä»¥ä¼˜å…ˆé€‰æ‹©é è¿‘**ç‚¹**çš„å¯¹å‡†ç‚¹ã€‚

![](img/96035d226c219681c9e967a770a8e255.png)

[source](https://arxiv.org/pdf/1508.04025.pdf)

### æ½œåœ¨çš„æ³¨æ„åŠ›

å°†è‡ªç„¶è¯­è¨€æè¿°ç¿»è¯‘æˆå¯æ‰§è¡Œç¨‹åºæ˜¯è®¡ç®—è¯­è¨€å­¦çš„ä¸€ä¸ªé‡è¦è¯¾é¢˜ã€‚åœ¨è¿‡å»çš„åå¹´é‡Œï¼Œè‡ªç„¶è¯­è¨€å¤„ç†å’Œç¼–ç¨‹è¯­è¨€ç¤¾åŒºä¸€ç›´åœ¨ç ”ç©¶è¿™ä¸ªä¸»é¢˜ã€‚ä¸ºäº†è¿™é¡¹ç ”ç©¶ï¼Œç ”ç©¶äººå‘˜åªå¯¹å¸¦æœ‰ä¸€ä¸ª if-then è¡¨è¾¾å¼çš„å­é›†ç¨‹åºæ„Ÿå…´è¶£ã€‚é…æ–¹æ˜¯æä¾›è§¦å‘æ¡ä»¶å’ŒåŠ¨ä½œåŠŸèƒ½çš„ If-Then ç¨‹åºï¼Œå®ƒä»¬è¡¨ç¤ºå½“è§¦å‘æ¡ä»¶æ»¡è¶³æ—¶æ‰§è¡Œçš„ç¨‹åºã€‚

LSTM æŠ€æœ¯ã€ç¥ç»ç½‘ç»œå’Œé€»è¾‘å›å½’æ–¹æ³•å·²ç»è¢«æå‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å¦ä¸€æ–¹é¢ï¼Œä½œè€…è®¤ä¸ºï¼Œè¯æ±‡å’Œå¥å‹çš„å¹¿æ³›å¤šæ ·æ€§ä½¿ RNN äººä¸å¯èƒ½å­¦ä¹ å¯ç”¨çš„è¡¨ç¤ºï¼Œä»–ä»¬çš„é›†æˆæŠ€æœ¯åœ¨å‡½æ•°é¢„æµ‹æŒ‘æˆ˜æ–¹é¢ç¡®å®ä¼˜äºåŸºäº LSTM çš„æ–¹æ³•ã€‚ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œä½œè€…æä¾›äº†ä¸€ç§æ–°çš„æ³¨æ„åŠ›ç»“æ„ï¼Œä»–ä»¬ç§°ä¹‹ä¸ºæ½œåœ¨æ³¨æ„åŠ›ã€‚æ½œåœ¨æ³¨æ„ä½¿ç”¨ä¸€ä¸ªåŠ æƒç³»ç»Ÿæ¥è¯„ä¼°æ¯ä¸ªæ ‡è®°åœ¨é¢„æµ‹è§¦å‘æˆ–è¡ŒåŠ¨ä¸­çš„é‡è¦æ€§ã€‚

#### æ½œåœ¨æ³¨æ„æ¨¡å‹

å¦‚æœæˆ‘ä»¬æƒ³æŠŠä¸€ä¸ªè‡ªç„¶è¯­è¨€æè¿°å˜æˆä¸€ä¸ªç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦åœ¨æè¿°ä¸­æ‰¾åˆ°å¯¹é¢„æµ‹é¢„æœŸæ ‡ç­¾æœ€æœ‰ç”¨çš„æœ¯è¯­ã€‚åœ¨ä¸‹é¢çš„æè¿°ä¸­æä¾›äº†å¯¹æ­¤çš„è¯´æ˜:

**è‡ªåŠ¨å°† Instagram ç…§ç‰‡ä¿å­˜åˆ° Dropbox æ–‡ä»¶å¤¹**

æœ¯è¯­â€œInstagram ç…§ç‰‡â€å°†å¸®åŠ©æˆ‘ä»¬æœ€å¥½åœ°é¢„æµ‹è§¦å‘å› ç´ ã€‚ä¸ºäº†è·å¾—è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å…¶é¦–å…ˆåŸºäºå¥å­ä¸­æ¯ä¸ªæ ‡è®°çš„é‡è¦æ€§è®¡ç®—æƒé‡ï¼Œç„¶åè¾“å‡ºè¿™äº›æ ‡è®°åµŒå…¥çš„åŠ æƒå’Œã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„ç›´è§‰å‘Šè¯‰æˆ‘ä»¬ï¼Œæ¯ä¸ªæ ‡è®°çš„é‡è¦æ€§å–å†³äºæ ‡è®°æœ¬èº«å’Œæ•´ä¸ªå¥å­çš„ç»“æ„ã€‚ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘ä¸‹é¢çš„å¥å­:

å°† Dropbox æ–‡ä»¶å¤¹ä¸­çš„ç…§ç‰‡å‘å¸ƒåˆ° Instagramã€‚

â€œDropboxâ€ç”¨äºå†³å®šè§¦å‘å™¨ï¼Œå°½ç®¡â€œInstagramâ€åº”è¯¥åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­æ‰®æ¼”è¿™ä¸ªè§’è‰²ï¼Œè¯¥ç¤ºä¾‹åŒ…æ‹¬å®é™…ä¸Šç›¸åŒçš„ä»¤ç‰Œé›†ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œä»‹è¯â€œtoâ€çš„ä½¿ç”¨ç»™äººçš„å°è±¡æ˜¯ï¼Œè§¦å‘é€šé“æ˜¯åœ¨è¿‡ç¨‹ä¸­é—´çš„æŸä¸ªåœ°æ–¹æè¿°çš„ï¼Œè€Œä¸æ˜¯åœ¨è¿‡ç¨‹ç»“æŸæ—¶ã€‚è€ƒè™‘åˆ°æ‰€æœ‰è¿™äº›å› ç´ ï¼Œæˆ‘ä»¬é€‰æ‹©äº†â€œDropboxâ€è€Œä¸æ˜¯â€œInstagramâ€ã€‚

æˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„æ³¨æ„æœºåˆ¶æ¥è®¡ç®—æ¯ä¸ªæ ‡è®°çš„æ½œåœ¨æƒé‡ï¼Œä»¥è¯†åˆ«åºåˆ—ä¸­çš„å“ªäº›æ ‡è®°ä¸è§¦å‘å™¨æˆ–åŠ¨ä½œæ›´ç›¸å…³ã€‚æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸»åŠ¨æƒé‡ï¼Œæ˜¯ç”±æ½œåœ¨æƒé‡å†³å®šçš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¡¨è¾¾å¼ä¸­å·²ç»å­˜åœ¨æ ‡è®°â€œto â€,æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹â€œtoâ€ä¹‹å‰çš„æ ‡è®°æ¥æœç´¢è§¦å‘å™¨ã€‚

### å˜å‹å™¨æ˜¯ä»€ä¹ˆï¼Ÿ

æ ¹æ®[è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)çš„è¯´æ³•ï¼Œtransformer æ˜¯ä¸€ç§æ¨¡å‹æ¶æ„ï¼Œå®ƒæ”¾å¼ƒäº†é€’å½’çš„ä½¿ç”¨ï¼Œè€Œæ˜¯åªä¾èµ–äºä¸€ç§æ³¨æ„åŠ›æœºåˆ¶æ¥ç¡®å®šè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å…¨å±€ç›¸äº’ä¾èµ–å…³ç³»ã€‚åœ¨å…«ä¸ª P100 GPU ä¸Šæ¥å—äº†çŸ­çŸ­ 12 ä¸ªå°æ—¶çš„åŸ¹è®­åï¼Œtransformer èƒ½å¤Ÿåœ¨ç¿»è¯‘è´¨é‡æ–¹é¢è¾¾åˆ°æ–°çš„è‰ºæœ¯æ°´å¹³ï¼Œå¹¶å…è®¸æ˜¾è‘—æé«˜å¹¶è¡Œæ€§ã€‚transformer æ˜¯ç¬¬ä¸€ä¸ªä»…é€šè¿‡è‡ªæˆ‘å…³æ³¨è€Œä¸æ˜¯åºåˆ—å¯¹é½ RNNs æˆ–å·ç§¯æ¥è®¡ç®—å…¶è¾“å…¥å’Œè¾“å‡ºè¡¨ç¤ºçš„è½¬å¯¼æ¨¡å‹ã€‚

### å•è¯åµŒå…¥

åµŒå…¥æ˜¯ç¬¦å·(å•è¯ã€å­—ç¬¦æˆ–çŸ­è¯­)åœ¨è¿ç»­å€¼å‘é‡çš„åˆ†å¸ƒå¼ä½ç»´ç©ºé—´ä¸­çš„è¡¨ç¤ºã€‚å•è¯ä¸æ˜¯å•ç‹¬çš„ç¬¦å·ã€‚ä»–ä»¬å½¼æ­¤ä¹‹é—´æœ‰ç€é‡è¦è€Œç§¯æçš„å…³ç³»ã€‚æ­£å› ä¸ºå¦‚æ­¤ï¼Œå½“æˆ‘ä»¬æŠŠå®ƒä»¬æŠ•å°„åˆ°ä¸€ä¸ªè¿ç»­çš„æ¬§å‡ é‡Œå¾—ç©ºé—´æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å®ƒä»¬ä¹‹é—´çš„å¯†åˆ‡å…³ç³»ã€‚ç„¶åï¼Œæ ¹æ®å·¥ä½œçš„ä¸åŒï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå°†å•è¯åµŒå…¥éš”å¼€ï¼Œæˆ–è€…ä½¿å®ƒä»¬å½¼æ­¤ç›¸å¯¹é è¿‘ã€‚

### æ¨¡å‹æ¶æ„

> å¤§å¤šæ•°ç«äº‰æ€§ç¥ç»åºåˆ—è½¬å¯¼æ¨¡å‹å…·æœ‰ç¼–ç å™¨-è§£ç å™¨ç»“æ„ã€‚è¿™é‡Œï¼Œç¼–ç å™¨å°†ç¬¦å·è¡¨ç¤ºçš„è¾“å…¥åºåˆ—(x1ï¼Œ...ï¼Œxn)è½¬æ¢ä¸ºè¿ç»­è¡¨ç¤ºåºåˆ— z = (z1ï¼Œ...ï¼Œzn)ã€‚ç»™å®š zï¼Œè§£ç å™¨ç„¶åç”Ÿæˆè¾“å‡ºåºåˆ—(y1ï¼Œ...ym)ï¼Œä¸€æ¬¡ä¸€ä¸ªå…ƒç´ ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæ¨¡å‹éƒ½æ˜¯è‡ªå›å½’çš„ï¼Œåœ¨ç”Ÿæˆä¸‹ä¸€æ­¥æ—¶ï¼Œæ¶ˆè€—å…ˆå‰ç”Ÿæˆçš„ç¬¦å·ä½œä¸ºé™„åŠ è¾“å…¥ã€‚

![](img/593bc890088493839ea185838c0e8e83.png)

[source](https://arxiv.org/pdf/1706.03762.pdf)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç¼–ç å™¨æ¨¡å‹åœ¨å·¦ä¾§ï¼Œè€Œè§£ç å™¨åœ¨å³ä¾§ã€‚æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œéƒ½å¯ä»¥åœ¨è¿™ä¸¤ä¸ªæ¨¡å—ä¸­æ‰¾åˆ°ã€‚

### è‡ªæˆ‘å…³æ³¨:åŸºæœ¬æ“ä½œ

æ ¹æ®è¿™ç¯‡[æ–‡ç« ](http://peterbloem.nl/blog/transformers)ï¼Œè‡ªæˆ‘å…³æ³¨æ˜¯ä¸€ä¸ªåºåˆ—å¯¹åºåˆ—çš„æ“ä½œã€‚å®ƒæ„å‘³ç€ä¸€ç³»åˆ—çš„å‘é‡è¢«è¾“å…¥åˆ°è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¦ä¸€ä¸ªå‘é‡åºåˆ—å°±å‡ºæ¥äº†ã€‚è®©æˆ‘ä»¬å°†è¾“å…¥å‘é‡ç§°ä¸º x1ã€x2ã€...å’Œ xtï¼Œè¾“å‡ºå‘é‡ä¸º y1ï¼Œy2ï¼Œ...ï¼Œè¿˜æœ‰ ytã€‚è‡ªå…³æ³¨æ“ä½œåªåŒ…æ‹¬å¯¹æ‰€æœ‰è¾“å…¥å‘é‡è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œä»¥è·å¾—è¾“å‡ºå‘é‡ **yi** ã€‚

![](img/5c177e4e5f2d0167397294efa25d0fde.png)

[Source](https://peterbloem.nl/blog/transformers)

> j å¯¹æ•´ä¸ªåºåˆ—è¿›è¡Œç´¢å¼•ï¼Œå¹¶ä¸”æƒé‡æ€»å’Œä¸º 1 å¯¹æ‰€æœ‰ jã€‚æƒé‡ **wij** ä¸æ˜¯ä¸€ä¸ªå‚æ•°ï¼Œå¦‚åŒåœ¨æ­£å¸¸çš„ç¥ç»ç½‘ç»œä¸­ä¸€æ ·ï¼Œä½†æ˜¯å®ƒæ˜¯ä»å¯¹ **ğ±i** å’Œğ±j.çš„å‡½æ•°ä¸­å¯¼å‡ºçš„ã€‚è¯¥å‡½æ•°æœ€ç®€å•çš„é€‰é¡¹æ˜¯ç‚¹ç§¯ã€‚

æˆ‘ä»¬å¿…é¡»åœ¨æ¨¡å‹çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶ä¸­åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªç»„ä»¶:æŸ¥è¯¢ã€å€¼å’Œé”®ã€‚

### æŸ¥è¯¢ã€å€¼å’Œé”®

ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæ˜¯é”®å€¼æŸ¥è¯¢æ¦‚å¿µçš„æ¥æºã€‚æ¾„æ¸…è¿™äº›æ¦‚å¿µå°†æ˜¯éå¸¸æœ‰ç›Šçš„ï¼Œå¹¶ä¸”é˜æ˜è¿™äº›å…³é”®æœ¯è¯­å°†æ˜¯éå¸¸æœ‰ç›Šçš„ã€‚ä»¥ YouTube ä¸ºä¾‹ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•æ‰¾åˆ°ä¸€ä¸ªè§†é¢‘ã€‚

æ‚¨å¯¹ç‰¹å®šè§†é¢‘çš„æœç´¢(æŸ¥è¯¢)å°†ä½¿æœç´¢å¼•æ“å°†æ‚¨çš„æœç´¢æ˜ å°„åˆ°ä¸€ç»„ä¸å¯èƒ½å·²å­˜å‚¨çš„è§†é¢‘ç›¸å…³çš„å…³é”®å­—(è§†é¢‘æ ‡é¢˜ã€æè¿°ç­‰)ã€‚ç„¶åï¼Œè¯¥ç®—æ³•å°†å‘æ‚¨æ˜¾ç¤ºæœ€æ¥è¿‘æ‚¨æ­£åœ¨å¯»æ‰¾çš„å†…å®¹(å€¼)çš„è§†é¢‘ã€‚ä¸ºäº†å¼•èµ·å˜å‹å™¨å¯¹è¿™ä¸€æ¦‚å¿µçš„æ³¨æ„ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹å†…å®¹:

![](img/439d9c48d68028e733904562599faa1c.png)

[Source](https://theaisummer.com/transformer/)

åœ¨æ£€ç´¢å•ä¸ªè§†é¢‘æ—¶ï¼Œæˆ‘ä»¬ä¾§é‡äºé€‰æ‹©å…·æœ‰æœ€é«˜å¯èƒ½ç›¸å…³æ€§åˆ†æ•°çš„è§†é¢‘ã€‚æ³¨æ„ç³»ç»Ÿå’Œæå–ç³»ç»Ÿä¹‹é—´æœ€å…³é”®çš„åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ›´æŠ½è±¡ã€æ›´ç®€åŒ–çš„â€œæå–â€é¡¹ç›®çš„æ¦‚å¿µã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡é¦–å…ˆç¡®å®šæˆ‘ä»¬çš„è¡¨ç¤ºä¹‹é—´å­˜åœ¨çš„ç›¸ä¼¼åº¦æˆ–æƒé‡æ¥åŠ æƒæˆ‘ä»¬çš„æŸ¥è¯¢ã€‚å› æ­¤ï¼Œæ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†æˆé”®-å€¼å¯¹ï¼Œå¹¶ä½¿ç”¨é”®æ¥ç¡®å®šæ³¨æ„åŠ›æƒé‡ï¼Œä»¥å°†å€¼å’Œæ•°æ®è§†ä¸ºæˆ‘ä»¬å°†è·å–çš„ä¿¡æ¯ã€‚

### æˆæ¯”ä¾‹çš„ç‚¹ç§¯æ³¨æ„åŠ›

æˆ‘ä»¬ç»™è¿™ç§ç‰¹æ®Šçš„æ³¨æ„åŠ›èµ·äº†ä¸ªåå­—ï¼Œå«åšâ€œæˆæ¯”ä¾‹çš„ç‚¹ç§¯æ³¨æ„åŠ›â€è¾“å…¥åŒ…æ‹¬:

*   æŸ¥è¯¢ï¼Œ
*   å°ºå¯¸ä¸º **dk** çš„é”®ï¼Œ
*   å°ºå¯¸å€¼ **dv** ã€‚

ä¸ºäº†è·å¾—å€¼çš„æƒé‡ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰é”®çš„ç‚¹ç§¯ï¼Œå°†æ¯ä¸ªé”®é™¤ä»¥ **âˆšdk** ï¼Œç„¶ååº”ç”¨ softmax å‡½æ•°ã€‚åœ¨å®é™…å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸€æ¬¡è®¡ç®—ä¸€ç»„æŸ¥è¯¢çš„å…³æ³¨åº¦å‡½æ•°ï¼Œç„¶åå°†è¿™äº›æŸ¥è¯¢æ‰“åŒ…åˆ°çŸ©é˜µ q ä¸­ã€‚å…³é”®å­—å’Œå€¼ä¹Ÿè¢«æ†ç»‘åˆ°çŸ©é˜µ K å’Œ v ä¸­ã€‚åˆ†æ•°æŒ‡ç¤ºç›¸å¯¹äºç‰¹å®šä½ç½®å¤„çš„ç‰¹å®šå•è¯ï¼Œåº”è¯¥ç»™äºˆè¾“å…¥åºåˆ—ä¸­çš„å…¶ä»–ä½ç½®æˆ–å•è¯å¤šå°‘æƒé‡ã€‚ä¸ºäº†æ¾„æ¸…ï¼Œå®ƒå°†æ˜¯æŸ¥è¯¢å‘é‡å’Œè¢«è¯„åˆ†çš„ç›¸åº”å•è¯çš„å…³é”®å‘é‡çš„ç‚¹ç§¯ã€‚å› æ­¤ï¼Œè¦å¾—åˆ°ç‚¹ç§¯(ã€‚)å¯¹äºä½ç½® 1ï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®— q1.k1ï¼Œç„¶åæ˜¯ q1.k2ã€q1.k3 ç­‰ç­‰...

![](img/511b828125aaf531aa23edf2aafaf25d.png)

[Source](https://arxiv.org/pdf/1706.03762.pdf)

å¯¹äºæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œæˆ‘ä»¬åº”ç”¨â€œç¼©æ”¾â€å› å­ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œsoftmax å‡½æ•°æ— æ³•æ­£ç¡®å¤„ç†å¤§å€¼ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œå¹¶å‡æ…¢å­¦ä¹ è¿‡ç¨‹ã€‚ä¹‹åï¼Œæˆ‘ä»¬ä½¿ç”¨å€¼çŸ©é˜µæ¥ä¿æŒæˆ‘ä»¬æƒ³è¦å…³æ³¨çš„å•è¯çš„å€¼ï¼Œå¹¶å‡å°‘æˆ–åˆ é™¤å¯¹æˆ‘ä»¬çš„ç›®çš„ä¸å¿…è¦çš„å•è¯çš„å€¼ã€‚

![](img/8ddd463ba3ba54568c7dfadee1ec65ac.png)

[Source](https://arxiv.org/pdf/1706.03762.pdf)

```py
def scaled_dot_product_attention(queries, keys, values, mask):
    # Calculate the dot product, QK_transpose
    product = tf.matmul(queries, keys, transpose_b=True)
    # Get the scale factor
    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)
    # Apply the scale factor to the dot product
    scaled_product = product / tf.math.sqrt(keys_dim)
    # Apply masking when it is requiered
    if mask is not None:
        scaled_product += (mask * -1e9)
    # dot product with Values
    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)

    return attention
```

### å¤šå¤´æ³¨æ„åŠ›

å³ä½¿ä¸¤ä¸ªå¥å­æœ‰ç›¸åŒçš„å•è¯ï¼Œä½†ä»¥ä¸åŒçš„é¡ºåºç»„ç»‡ï¼Œæ³¨æ„åŠ›åˆ†æ•°ä»ç„¶ä¼šæä¾›ç›¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³æŠŠæ³¨æ„åŠ›é›†ä¸­åœ¨å•è¯çš„ä¸åŒéƒ¨åˆ†ã€‚

> ç†è§£å¤šå¤´è‡ªæˆ‘å…³æ³¨çš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†å…¶è§†ä¸ºå¹¶è¡Œåº”ç”¨çš„è‡ªæˆ‘å…³æ³¨æœºåˆ¶çš„å°‘é‡å‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬éƒ½æœ‰è‡ªå·±çš„é”®ã€å€¼å’ŒæŸ¥è¯¢è½¬æ¢ã€‚

![](img/12bbf3ab1e9dc83ffa91d1b6c3539002.png)

[Source](https://arxiv.org/pdf/1706.03762.pdf)

è¯¥æ¨¡å‹å¯ä»¥ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›åœ¨ä¸åŒç‚¹å…³æ³¨æ¥è‡ªä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ã€‚é€šè¿‡ä½¿ç”¨ Qã€K å’Œ V å­çŸ©é˜µå°†å•è¯å‘é‡åˆ†æˆå®šä¹‰æ•°é‡çš„ç»„å—ï¼Œç„¶åå°†è‡ªæˆ‘æ³¨æ„åº”ç”¨äºç›¸åº”çš„ç»„å—ï¼Œå¯ä»¥å¢å¼ºè‡ªæˆ‘æ³¨æ„åŒºåˆ†ä¸åŒå•è¯çš„èƒ½åŠ›ã€‚ä¸€æ—¦æˆ‘ä»¬è®¡ç®—äº†æ¯ä¸ªå¤´éƒ¨çš„ç‚¹ç§¯ï¼Œæˆ‘ä»¬å°±è¿æ¥è¾“å‡ºçŸ©é˜µï¼Œå¹¶æŠŠå®ƒä»¬ä¹˜ä»¥ä¸€ä¸ªæƒé‡çŸ©é˜µã€‚

![](img/2e49143d8a20c366ed8ab16f1c8a799b.png)

[Source](https://arxiv.org/pdf/1706.03762.pdf)

```py
class MultiHeadAttention(layers.Layer):

    def __init__(self, n_heads):
        super(MultiHeadAttention, self).__init__()
        self.n_heads = n_heads

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        assert self.d_model % self.n_heads == 0
        # Calculate the dimension of every head or projection
        self.d_head = self.d_model // self.n_heads
        # Set the weight matrices for Q, K and V
        self.query_lin = layers.Dense(units=self.d_model)
        self.key_lin = layers.Dense(units=self.d_model)
        self.value_lin = layers.Dense(units=self.d_model)
        # Set the weight matrix for the output of the multi-head attention W0
        self.final_lin = layers.Dense(units=self.d_model)

    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)
        # Set the dimension of the projections
        shape = (batch_size,
                 -1,
                 self.n_heads,
                 self.d_head)
        # Split the input vectors
        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)
        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)

    def call(self, queries, keys, values, mask):
        # Get the batch size
        batch_size = tf.shape(queries)[0]
        # Set the Query, Key and Value matrices
        queries = self.query_lin(queries)
        keys = self.key_lin(keys)
        values = self.value_lin(values)
        # Split Q, K y V between the heads or projections
        queries = self.split_proj(queries, batch_size)
        keys = self.split_proj(keys, batch_size)
        values = self.split_proj(values, batch_size)
        # Apply the scaled dot product
        attention = scaled_dot_product_attention(queries, keys, values, mask)
        # Get the attention scores
        attention = tf.transpose(attention, perm=[0, 2, 1, 3])
        # Concat the h heads or projections
        concat_attention = tf.reshape(attention,
                                      shape=(batch_size, -1, self.d_model))
        # Apply W0 to get the output of the multi-head attention
        outputs = self.final_lin(concat_attention)

        return outputs
```

### ä½ç½®ç¼–ç 

åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­æ²¡æœ‰é€’å½’å’Œå·ç§¯ï¼Œæœ‰å¿…è¦æä¾›ä¸€äº›å…³äºè®°å·åœ¨åºåˆ—ä¸­å¦‚ä½•ç›¸äº’å®šä½çš„ä¿¡æ¯ã€‚â€œä½ç½®ç¼–ç â€è¢«æ·»åŠ åˆ°ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆåº•éƒ¨çš„åµŒå…¥ä¸­ï¼Œä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚å› ä¸ºä½ç½®ç¼–ç å’ŒåµŒå…¥éƒ½æœ‰ç›¸åŒçš„ç»´æ•°ï¼Œæ‰€ä»¥å¯ä»¥å°†ä¸¤ä¸ªå€¼ç›¸åŠ ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå‡½æ•°å°†å¥å­ä¸­çš„ä½ç½®è½¬æ¢æˆä¸€ä¸ªå®å€¼å‘é‡ã€‚æœ€ç»ˆï¼Œç½‘ç»œå°†ä¼šå¼„æ¸…æ¥šå¦‚ä½•ä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚

ä½ç½®åµŒå…¥å¯ä»¥åƒå•è¯åµŒå…¥ä¸€æ ·ä½¿ç”¨ï¼Œå…¶ä¸­æ¯ä¸ªå·²çŸ¥çš„ä½ç½®ç”¨ä¸€ä¸ªå‘é‡ç¼–ç ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°è¢«åº”ç”¨äº:

![](img/ce75c1f28478bfd9e2382ccd57c3c54b.png)

[Source](https://arxiv.org/pdf/1706.03762.pdf)

transformer ç ”ç©¶çš„ä½œè€…åœ¨ transformer è®ºæ–‡ä¸­æå‡ºäº†ä¸€ä¸ªç”¨äºä½ç½®ç¼–ç çš„æ­£å¼¦å‡½æ•°ã€‚æ­£å¼¦å‡½æ•°æŒ‡ç¤ºæ¨¡å‹èšç„¦äºç‰¹å®šæ³¢é•¿ã€‚ä¿¡å·çš„æ³¢é•¿ y(x)=sin(kx)ä¸º k=2Ï€ /Î»ã€‚Î»å°†ç”±å¥å­çš„ä½ç½®å†³å®šã€‚å€¼ I è¡¨ç¤ºä¸€ä¸ªä½ç½®æ˜¯å¥‡æ•°è¿˜æ˜¯å¶æ•°ã€‚

```py
class PositionalEncoding(layers.Layer):

    def __init__(self):
        super(PositionalEncoding, self).__init__()

    def get_angles(self, pos, i, d_model): # pos: (seq_length, 1) i: (1, d_model)
        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))
        return pos * angles # (seq_length, d_model)

    def call(self, inputs):
        # input shape batch_size, seq_length, d_model
        seq_length = inputs.shape.as_list()[-2]
        d_model = inputs.shape.as_list()[-1]
        # Calculate the angles given the input
        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],
                                 np.arange(d_model)[np.newaxis, :],
                                 d_model)
        # Calculate the positional encodings
        angles[:, 0::2] = np.sin(angles[:, 0::2])
        angles[:, 1::2] = np.cos(angles[:, 1::2])
        # Expand the encodings with a new dimension
        pos_encoding = angles[np.newaxis, ...]

        return inputs + tf.cast(pos_encoding, tf.float32)
```

### ç¼–ç å™¨

[ç¯‡è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)ä»‹ç»ç¼–ç å™¨ç»„ä»¶å¦‚ä¸‹:

*   ç¼–ç å™¨åŒ…æ‹¬å…·æœ‰ N = 6 ä¸ªç›¸åŒå±‚çš„å †æ ˆã€‚
*   æ¯å±‚æœ‰ä¸¤ä¸ªå­å±‚ã€‚ç¬¬ä¸€ä¸ªæ˜¯å¤šå¤´è‡ªå…³æ³¨æœºåˆ¶ï¼Œç¬¬äºŒä¸ªæ˜¯å…¨è¿æ¥å‰é¦ˆç½‘ç»œã€‚
*   åœ¨ä¸¤ä¸ªå­å±‚çš„æ¯ä¸€ä¸ªå‘¨å›´ä½¿ç”¨å‰©ä½™è¿æ¥ï¼Œéšåè¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚æ¯ä¸ªå­å±‚çš„è¾“å‡ºæ˜¯ LayerNorm(x + Sublayer(x))ï¼Œå…¶ä¸­ Sublayer(x)æ˜¯å­å±‚æœ¬èº«å®ç°çš„å‡½æ•°ã€‚

![](img/943d51265ad5bbe9bc276ea9c08ddfb9.png)

[Source](https://jalammar.github.io/illustrated-transformer/)

å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥æ˜¯ä¸¤ç§æœ€å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç”¨äºåŠ é€Ÿå’Œæé«˜æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒçš„å‡†ç¡®æ€§ã€‚åªæœ‰åµŒå…¥ç»´åº¦å—å±‚è§„èŒƒåŒ–çš„å½±å“ã€‚å¯¹äºæ¯ä¸ªå­å±‚ï¼Œæˆ‘ä»¬åœ¨å°†å…¶ä¸å­å±‚è¾“å…¥å½’ä¸€åŒ–å’Œç»„åˆä¹‹å‰ï¼Œå¯¹è¾“å‡ºåº”ç”¨ä¸‹é™ã€‚ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆåµŒå…¥å’Œä½ç½®ç¼–ç å’Œéƒ½è¢«ä¸¢å¼ƒã€‚

æˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨æ‰¹å¤„ç†è§„èŒƒåŒ–ï¼Œå› ä¸ºå®ƒéœ€è¦å¤§é‡çš„ GPU RAMã€‚BatchNorm å·²ç»è¢«è¯æ˜åœ¨è¯­è¨€ä¸­è¡¨ç°ç‰¹åˆ«å·®ï¼Œå› ä¸ºå•è¯çš„ç‰¹å¾å¾€å¾€å…·æœ‰æ˜¾è‘—æ›´é«˜çš„æ–¹å·®(è®¸å¤šç¨€ç¼ºå•è¯éœ€è¦è¢«è€ƒè™‘ç”¨äºåˆç†çš„åˆ†å¸ƒä¼°è®¡)ã€‚

**æ³¨æ„**:å’Œ ResNets ä¸€æ ·ï¼Œå˜å½¢é‡‘åˆšä¹Ÿå€¾å‘äºæ·±å…¥ã€‚åœ¨ç‰¹å®šå‹å·ä¸­å¯èƒ½ä¼šå‘ç°è¶…è¿‡ 24 å—ã€‚åªæœ‰å½“æ¨¡å‹å…·æœ‰å‰©ä½™è¿æ¥æ—¶ï¼Œæ‰èƒ½å‡ºç°å¹³æ»‘çš„æ¢¯åº¦æµã€‚æˆ‘ä»¬å°†ç»§ç»­å®æ–½ç¼–ç å™¨å±‚:

```py
class EncoderLayer(layers.Layer):

    def __init__(self, FFN_units, n_heads, dropout_rate):
        super(EncoderLayer, self).__init__()
        # Hidden units of the feed forward component
        self.FFN_units = FFN_units
        # Set the number of projectios or heads
        self.n_heads = n_heads
        # Dropout rate
        self.dropout_rate = dropout_rate

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        # Build the multihead layer
        self.multi_head_attention = MultiHeadAttention(self.n_heads)
        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)
        # Layer Normalization
        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)
        # Fully connected feed forward layer
        self.ffn1_relu = layers.Dense(units=self.FFN_units, activation="relu")
        self.ffn2 = layers.Dense(units=self.d_model)
        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)
        # Layer normalization
        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)

    def call(self, inputs, mask, training):
        # Forward pass of the multi-head attention
        attention = self.multi_head_attention(inputs,
                                              inputs,
                                              inputs,
                                              mask)
        attention = self.dropout_1(attention, training=training)
        # Call to the residual connection and layer normalization
        attention = self.norm_1(attention + inputs)
        # Call to the FC layer
        outputs = self.ffn1_relu(attention)
        outputs = self.ffn2(outputs)
        outputs = self.dropout_2(outputs, training=training)
        # Call to residual connection and the layer normalization
        outputs = self.norm_2(outputs + attention)

        return outputs
```

å’Œç¼–ç å™¨å®ç°:

```py
class Encoder(layers.Layer):

    def __init__(self,
                 n_layers,
                 FFN_units,
                 n_heads,
                 dropout_rate,
                 vocab_size,
                 d_model,
                 name="encoder"):
        super(Encoder, self).__init__(name=name)
        self.n_layers = n_layers
        self.d_model = d_model
        # The embedding layer
        self.embedding = layers.Embedding(vocab_size, d_model)
        # Positional encoding layer
        self.pos_encoding = PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout_rate)
        # Stack of n layers of multi-head attention and FC
        self.enc_layers = [EncoderLayer(FFN_units,
                                        n_heads,
                                        dropout_rate) 
                           for _ in range(n_layers)]

    def call(self, inputs, mask, training):
        # Get the embedding vectors
        outputs = self.embedding(inputs)
        # Scale the embeddings by sqrt of d_model
        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        # Positional encodding
        outputs = self.pos_encoding(outputs)
        outputs = self.dropout(outputs, training)
        # Call the stacked layers
        for i in range(self.n_layers):
            outputs = self.enc_layers[i](outputs, mask, training)

        return outputs
```

### è§£ç å™¨

ç¼–ç å™¨å’Œè§£ç å™¨éƒ½åŒ…æ‹¬ä¸€äº›ç›¸åŒçš„ç»„ä»¶ï¼Œä½†æ˜¯è¿™äº›ç»„ä»¶åœ¨è§£ç å™¨ä¸­è¢«ä¸åŒåœ°ä½¿ç”¨ï¼Œä»¥ä¾¿å®ƒå¯ä»¥è€ƒè™‘ç¼–ç å™¨çš„è¾“å‡ºã€‚

*   è§£ç å™¨ä¹Ÿç”± N = 6 ä¸ªç›¸åŒå±‚çš„å †å ç»„æˆã€‚
*   é™¤äº†æ¯ä¸ªç¼–ç å™¨å±‚ä¸­çš„ä¸¤ä¸ªå­å±‚ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œè¯¥å­å±‚å¯¹ç¼–ç å™¨å †æ ˆçš„è¾“å‡ºæ‰§è¡Œå¤šå¤´å…³æ³¨ã€‚
*   åƒç¼–ç å™¨ä¸€æ ·ï¼Œåœ¨æ¯ä¸ªå­å±‚å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œç„¶åè¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
*   åœ¨è§£ç å™¨å †æ ˆä¸­ä¿®æ”¹è‡ªå…³æ³¨å­å±‚ï¼Œä»¥é˜²æ­¢ä½ç½®å…³æ³¨åç»­ä½ç½®ã€‚è¿™ç§å±è”½ä¸è¾“å‡ºåµŒå…¥ç§»åŠ¨ä¸€ä¸ªä½ç½®çš„äº‹å®ç›¸ç»“åˆï¼Œä¿è¯äº†ä½ç½® I çš„é¢„æµ‹åªèƒ½ä¾èµ–äºå·²çŸ¥å‡ºç°åœ¨ä½äº I çš„ä½ç½®çš„è¾“å‡ºã€‚

![](img/8a097bd610aab01c6765cebab6734623.png)

[Source](http://jalammar.github.io/illustrated-transformer/)

è§£ç å™¨å±‚å®ç°:

```py
class DecoderLayer(layers.Layer):

    def __init__(self, FFN_units, n_heads, dropout_rate):
        super(DecoderLayer, self).__init__()
        self.FFN_units = FFN_units
        self.n_heads = n_heads
        self.dropout_rate = dropout_rate

    def build(self, input_shape):
        self.d_model = input_shape[-1]

        # Self multi head attention, causal attention
        self.multi_head_causal_attention = MultiHeadAttention(self.n_heads)
        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)
        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)

        # Multi head attention, encoder-decoder attention 
        self.multi_head_enc_dec_attention = MultiHeadAttention(self.n_heads)
        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)
        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)

        # Feed foward
        self.ffn1_relu = layers.Dense(units=self.FFN_units,
                                    activation="relu")
        self.ffn2 = layers.Dense(units=self.d_model)
        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)
        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        # Call the masked causal attention
        attention = self.multi_head_causal_attention(inputs,
                                                inputs,
                                                inputs,
                                                mask_1)
        attention = self.dropout_1(attention, training)
        # Residual connection and layer normalization
        attention = self.norm_1(attention + inputs)
        # Call the encoder-decoder attention
        attention_2 = self.multi_head_enc_dec_attention(attention,
                                                  enc_outputs,
                                                  enc_outputs,
                                                  mask_2)
        attention_2 = self.dropout_2(attention_2, training)
        # Residual connection and layer normalization
        attention_2 = self.norm_2(attention_2 + attention)
        # Call the Feed forward
        outputs = self.ffn1_relu(attention_2)
        outputs = self.ffn2(outputs)
        outputs = self.dropout_3(outputs, training)
        # Residual connection and layer normalization
        outputs = self.norm_3(outputs + attention_2)

        return outputs
```

çº¿æ€§å±‚å°†å †å è¾“å‡ºè½¬æ¢ä¸ºä¸€ä¸ªæ›´å¤§çš„å‘é‡ï¼Œç§°ä¸º logitsï¼Œè¿™æ˜¯ä¸€ä¸ªå®Œå…¨è¿æ¥çš„ç½‘ç»œï¼Œä½äº N ä¸ªå †å è§£ç å™¨çš„æœ«ç«¯ã€‚å¯¹æ•°é€šè¿‡ softmax å±‚è½¬æ¢æˆæ¦‚ç‡ã€‚

è§£ç å™¨çš„å®ç°:

```py
class Decoder(layers.Layer):

    def __init__(self,
                 n_layers,
                 FFN_units,
                 n_heads,
                 dropout_rate,
                 vocab_size,
                 d_model,
                 name="decoder"):
        super(Decoder, self).__init__(name=name)
        self.d_model = d_model
        self.n_layers = n_layers
        # Embedding layer
        self.embedding = layers.Embedding(vocab_size, d_model)
        # Positional encoding layer
        self.pos_encoding = PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout_rate)
        # Stacked layers of multi-head attention and feed forward
        self.dec_layers = [DecoderLayer(FFN_units,
                                        n_heads,
                                        dropout_rate) 
                           for _ in range(n_layers)]

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        # Get the embedding vectors
        outputs = self.embedding(inputs)
        # Scale by sqrt of d_model
        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        # Positional encodding
        outputs = self.pos_encoding(outputs)
        outputs = self.dropout(outputs, training)
        # Call the stacked layers
        for i in range(self.n_layers):
            outputs = self.dec_layers[i](outputs,
                                         enc_outputs,
                                         mask_1,
                                         mask_2,
                                         training)

        return outputs
```

### ç»“è®º

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å˜å‹å™¨çš„ç»„ä»¶ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨åŸºäºæ³¨æ„åŠ›çš„åºåˆ—è½¬å¯¼æ¨¡å‹ã€‚è¯¥æ¨¡å‹ç”¨å¤šå¤´è‡ªå…³æ³¨å–ä»£äº†ç¼–ç å™¨-è§£ç å™¨è®¾è®¡ä¸­æœ€å¸¸ç”¨çš„é€’å½’å±‚ã€‚åœ¨ä¸‹ä¸€ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®º BERTï¼Œå®ƒæ˜¯ç¬¬ä¸€ä¸ªè¡¨æ˜å˜å½¢é‡‘åˆšå¯ä»¥è¾¾åˆ°äººç±»æ°´å¹³æ€§èƒ½çš„æ¨¡å‹ä¹‹ä¸€ã€‚

### å‚è€ƒ

[https://theaisummer.com/transformer/](https://theaisummer.com/transformer/)
æœ¬æ–‡ä½¿ç”¨çš„ä»£ç æ¥è‡ª[https://towardsdatascience . com/attention-is-all-you-need-discovering-The-transformer-paper-73 e5f F5 e 0634](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634)
[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
[https://peterbloem.nl/blog/transformers](https://peterbloem.nl/blog/transformers)
[https://UVA DLC-notebooks . io/en/latest/tutorial 6/Transformers _ and _ MH attention . https://UVA DLC-notebooks . read docs . io/end](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)